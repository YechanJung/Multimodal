{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install kaggle transformers sentence-transformers torch torchvision numpy matplotlib pillow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instructions for MacOS/Linux Machine\n",
    "!mkdir ~/.kaggle\n",
    "!copy kaggle.json ~/.kaggle/\n",
    "!chmod 600 ~/.kaggle/kaggle.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!kaggle datasets download sakshighadigaonkar/flickr-8k\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!unzip flickr-8k.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# libraries\n",
    "import numpy as np\n",
    "from numpy import array\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import string\n",
    "import os\n",
    "from PIL import Image\n",
    "from time import time\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_path = \"./flickr-8k/Flickr8k_text/Flickr8k.token.txt\"\n",
    "train_images_path = './flickr-8k/Flickr8k_text/Flickr_8k.trainImages.txt'\n",
    "test_images_path = './flickr-8k/Flickr8k_text/Flickr_8k.testImages.txt'\n",
    "images_path = './flickr-8k/Flickr8k_Dataset/Flicker8k_Dataset'\n",
    "\n",
    "doc = open(token_path,'r').read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  NLP setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 임베딩 가져오기\n",
    "from sentence_transformers import SentenceTransformer, AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"nvidia/NV-Embed-v2\")\n",
    "embedding_model = SentenceTransformer('nvidia/NV-Embed-v2', trust_remote_code=True)\n",
    "embedding_model.max_seq_length = 32768\n",
    "embedding_model.tokenizer.padding_side=\"right\"\n",
    "def add_eos(input_examples):\n",
    "  input_examples = [input_example + embedding_model.tokenizer.eos_token for input_example in input_examples]\n",
    "  return input_examples\n",
    "vocab_size = 32000\n",
    "max_length = 32\n",
    "num_photos_per_batch = 1000\n",
    "pad_sequences = tokenizer.pad_sequences\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# 이미지 description 가져오기\n",
    "from pathlib import Path\n",
    "descriptions = dict()\n",
    "for line in doc.split('\\n'):\n",
    "        tokens = line.split()\n",
    "        if len(line) > 2:\n",
    "          image_id = tokens[0].split('.')[0]\n",
    "          image_desc = ' '.join(tokens[1:])\n",
    "          if image_id not in descriptions:\n",
    "              descriptions[image_id] = list()\n",
    "          descriptions[image_id].append(image_desc)\n",
    "\n",
    "# 이미지 description 조작하기\n",
    "lines = list()\n",
    "for key, desc_list in descriptions.items():\n",
    "    for desc in desc_list:\n",
    "        lines.append(key + ' ' + desc)\n",
    "new_descriptions = '\\n'.join(lines)\n",
    "\n",
    "# train 이미지 가져오기\n",
    "doc = open(train_images_path,'r').read()\n",
    "dataset = list()\n",
    "for line in doc.split('\\n'):\n",
    "    if len(line) > 1:\n",
    "      identifier = line.split('.')[0]\n",
    "      dataset.append(identifier)\n",
    "\n",
    "train = set(dataset)\n",
    "\n",
    "\n",
    "# train 이미지 description 가져오기\n",
    "train_descriptions = dict()\n",
    "for line in descriptions.split('\\n'):\n",
    "    tokens = line.split()\n",
    "    image_id, image_desc = tokens[0], tokens[1:]\n",
    "    if image_id in train:\n",
    "        if image_id not in train_descriptions:\n",
    "            train_descriptions[image_id] = list()\n",
    "        desc = '<s> ' + ' '.join(image_desc) + ' </s>' # adding the start and end sequence tokens for latter transformation\n",
    "        train_descriptions[image_id].append(desc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Encoding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#In this case we are going to actually get the images and stored them in train_img list\n",
    "img = list(Path(images_path).glob('*.jpg'))\n",
    "train_images = set(open(train_images_path, 'r').read().strip().split('\\n'))\n",
    "train_img = []\n",
    "for i in img:\n",
    "    if i.split('/')[-1].split('\\\\')[-1] in train_images: ## .split('\\\\')[-1] this is added if you run this in a windows machine. Remove it if you are running in MacOS/Linux machine\n",
    "        train_img.append(i)\n",
    "\n",
    "test_images = set(open(test_images_path, 'r').read().strip().split('\\n'))\n",
    "test_img = []\n",
    "for i in img: \n",
    "    if i.split('/')[-1].split('\\\\')[-1] in test_images:  ## .split('\\\\')[-1] this is added if you run this in a windows machine. Remove it if you are running in MacOS/Linux machine\n",
    "        test_img.append(i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_image(image_path):\n",
    "    image = Image.open(image_path)\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ])\n",
    "    return transform(image).unsqueeze(0)\n",
    "\n",
    "vit_model = models.vit_b_32(pretrained=True)\n",
    "vit = nn.Sequential(*list(vit_model.children())[:-1]) \n",
    "def encode_image(image_path):\n",
    "    image = preprocess_image(image_path)\n",
    "    with torch.no_grad():\n",
    "        image = vit(image)\n",
    "    return image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_DL = torch.utils.data.DataLoader()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DataLoader 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generates the actual input pair and output by combining each photo with each of its captions and use another caption as output. \n",
    "def data_generator(descriptions, photos, wordtoix, max_length, num_photos_per_batch):\n",
    "    X1, X2, y = list(), list(), list()\n",
    "    n=0\n",
    "    # loop for ever over images\n",
    "    while 1:\n",
    "        for key, desc_list in descriptions.items():\n",
    "            n+=1\n",
    "            # retrieve the photo feature\n",
    "            photo = photos[images_path + '\\\\' + key + '.jpg']\n",
    "            for desc in desc_list:\n",
    "                # encode the sequence\n",
    "                seq = [wordtoix[word] for word in desc.split(' ') if word in wordtoix]\n",
    "                # split one sequence into multiple X, y pairs\n",
    "                for i in range(1, len(seq)):\n",
    "                    # split into input and output pair\n",
    "                    in_seq, out_seq = seq[:i], seq[i]\n",
    "                    # pad input sequence\n",
    "                    in_seq = pad_sequences([in_seq], maxlen=max_length)[0]\n",
    "                    # encode output sequence\n",
    "                    out_seq = F.onehot([out_seq], num_classes=vocab_size)[0]\n",
    "                    # store\n",
    "                    X1.append(photo)\n",
    "                    X2.append(in_seq)\n",
    "                    y.append(out_seq)\n",
    "\n",
    "            if n==num_photos_per_batch:\n",
    "                yield ([array(X1), array(X2)], array(y))\n",
    "                X1, X2, y = list(), list(), list()\n",
    "                n=0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Captioning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Image_Caption(nn.Module):\n",
    "    def __init__(self, drop_p=0.5, image_dim=256, vocab_size=32000, embedding_dim=4096, hidden_dim=1024):\n",
    "        super(Image_Caption, self).__init__()\n",
    "        self.image_encoder = nn.Sequential(\n",
    "            nn.Dropout(drop_p),\n",
    "            nn.Linear(image_dim, 256),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        self.embedding = embedding_model\n",
    "        self.text_dropout = nn.Dropout(drop_p)\n",
    "        self.linear = nn.Linear(embedding_dim, hidden_dim)\n",
    "        self.lstm = nn.LSTM(hidden_dim, 256, batch_first=True)\n",
    "\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(256, vocab_size),\n",
    "            nn.ReLu(),\n",
    "            nn.Linear(vocab_size, vocab_size),\n",
    "            nn.Softmax(dim=1)\n",
    "        )\n",
    "    def forward(self, image_features, text):\n",
    "        image_features = self.image_encoder(image_features)\n",
    "        with torch.no_grad():\n",
    "            text_features = self.embedding.encode(add_eos(text), batch_size=1, prompt=False, normalize_embeddings=True)\n",
    "        text_features = self.text_dropout(text_features)\n",
    "        text_features = self.linear(text_features)\n",
    "        lstm_out, _ = self.lstm(text_features)\n",
    "        text_features = lstm_out[:, -1, :]\n",
    "        fused_features = image_features + text_features\n",
    "\n",
    "        output = self.decoder(fused_features)\n",
    "        return output\n",
    "    \n",
    "model = Image_Caption()\n",
    "\n",
    "vocab_size = 32000\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordtoix = tokenizer(word_index=True)\n",
    "wordtoix = {k: v+1 for k, v in wordtoix.items()}\n",
    "generator = data_generator(train_descriptions, train_img, wordtoix, max_length, num_photos_per_batch)\n",
    "model.fit(generator, epochs=15, steps_per_epoch=100, verbose=1)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
