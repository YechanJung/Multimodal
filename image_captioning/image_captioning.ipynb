{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "!pip install kaggle transformers sentence-transformers torch torchvision numpy matplotlib pillow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!kaggle datasets download sakshighadigaonkar/flickr-8k\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# libraries\n",
    "import numpy as np\n",
    "from numpy import array\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import string\n",
    "import os\n",
    "from PIL import Image\n",
    "from time import time\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_path = \"./flickr-8k/Flickr8k_text/Flickr8k.token.txt\"\n",
    "train_images_path = './flickr-8k/Flickr8k_text/Flickr_8k.trainImages.txt'\n",
    "test_images_path = './flickr-8k/Flickr8k_text/Flickr_8k.testImages.txt'\n",
    "images_path = './flickr-8k/Flickr8k_Dataset/Flicker8k_Dataset'\n",
    "\n",
    "doc = open(token_path,'r').read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "descriptions = dict()\n",
    "for line in doc.split('\\n'):\n",
    "        tokens = line.split()\n",
    "        if len(line) > 2:\n",
    "          image_id = tokens[0].split('.')[0]\n",
    "          image_desc = ' '.join(tokens[1:])\n",
    "          if image_id not in descriptions:\n",
    "              descriptions[image_id] = list()\n",
    "          descriptions[image_id].append(image_desc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary = set()\n",
    "for key in descriptions.keys():\n",
    "        [vocabulary.update(d.split()) for d in descriptions[key]]\n",
    "print('Original Vocabulary Size: %d words' % len(vocabulary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Manipulation of the descriptions to prepare them to join them with their correspondent images\n",
    "lines = list()\n",
    "for key, desc_list in descriptions.items():\n",
    "    for desc in desc_list:\n",
    "        lines.append(key + ' ' + desc)\n",
    "new_descriptions = '\\n'.join(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Collects in train the images we are going to use for training purposes.\n",
    "doc = open(train_images_path,'r').read()\n",
    "dataset = list()\n",
    "for line in doc.split('\\n'):\n",
    "    if len(line) > 1:\n",
    "      identifier = line.split('.')[0]\n",
    "      dataset.append(identifier)\n",
    "\n",
    "train = set(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#In this case we are going to actually get the images and stored them in train_img list\n",
    "import glob\n",
    "\n",
    "img = glob.glob(images_path + '/*.jpg')\n",
    "train_images = set(open(train_images_path, 'r').read().strip().split('\\n'))\n",
    "train_img = []\n",
    "for i in img:\n",
    "    if i.split('/')[-1].split('\\\\')[-1] in train_images: ## .split('\\\\')[-1] this is added if you run this in a windows machine. Remove it if you are running in MacOS/Linux machine\n",
    "        train_img.append(i)\n",
    "\n",
    "test_images = set(open(test_images_path, 'r').read().strip().split('\\n'))\n",
    "test_img = []\n",
    "for i in img: \n",
    "    if i.split('/')[-1].split('\\\\')[-1] in test_images:  ## .split('\\\\')[-1] this is added if you run this in a windows machine. Remove it if you are running in MacOS/Linux machine\n",
    "        test_img.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_descriptions = dict()\n",
    "for line in new_descriptions.split('\\n'):\n",
    "    tokens = line.split()\n",
    "    image_id, image_desc = tokens[0], tokens[1:]\n",
    "    if image_id in train:\n",
    "        if image_id not in train_descriptions:\n",
    "            train_descriptions[image_id] = list()\n",
    "        desc = 'startseq ' + ' '.join(image_desc) + ' endseq' # adding the start and end sequence tokens for latter transformation\n",
    "        train_descriptions[image_id].append(desc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_train_captions = []\n",
    "for key, val in train_descriptions.items():\n",
    "    for cap in val:\n",
    "        all_train_captions.append(cap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Filtering out less relevant words from our description vocabulary. \n",
    "word_count_threshold = 10\n",
    "word_counts = {}\n",
    "nsents = 0\n",
    "for sent in all_train_captions:\n",
    "    nsents += 1\n",
    "    for w in sent.split(' '):\n",
    "        word_counts[w] = word_counts.get(w, 0) + 1\n",
    "        \n",
    "vocab = [w for w in word_counts if word_counts[w] >= word_count_threshold]\n",
    "\n",
    "print('Vocabulary szie for words that have 10 or more occurrences= %d' % (len(vocab)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we build up our mapping to map each word to an index.\n",
    "\n",
    "ixtoword = {}\n",
    "wordtoix = {}\n",
    "ix = 1\n",
    "for w in vocab:\n",
    "    wordtoix[w] = ix\n",
    "    ixtoword[ix] = w\n",
    "    ix += 1\n",
    "\n",
    "vocab_size = len(ixtoword) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we capture the longest description in our description data set.\n",
    "all_desc = list()\n",
    "for key in train_descriptions.keys():\n",
    "    [all_desc.append(d) for d in train_descriptions[key]]\n",
    "lines = all_desc\n",
    "max_length = max(len(d.split()) for d in lines)\n",
    "\n",
    "print('Description Length: %d' % max_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 임베딩 가져오기\n",
    "from sentence_transformers import SentenceTransformer, AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"nvidia/NV-Embed-v2\")\n",
    "embedding_model = SentenceTransformer('nvidia/NV-Embed-v2', trust_remote_code=True)\n",
    "embedding_model.max_seq_length = 32768\n",
    "embedding_model.tokenizer.padding_side=\"right\"\n",
    "def add_eos(input_examples):\n",
    "  input_examples = [input_example + embedding_model.tokenizer.eos_token for input_example in input_examples]\n",
    "  return input_examples\n",
    "\n",
    "\n",
    "\n",
    "embeddings_index = {} \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Encoding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vision_model = models.vit_b_32(pretrained=True)\n",
    "vit = nn.Sequential(*list(vision_model.children())[:-1]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_image(image_path):\n",
    "    image = Image.open(image_path)\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ])\n",
    "    return transform(image).unsqueeze(0)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_img = train_img[:100]\n",
    "test_img = test_img[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_image(image_path):\n",
    "    image = preprocess_image(image_path)\n",
    "    with torch.no_grad():\n",
    "        image = vit(image)\n",
    "    return image\n",
    "\n",
    "encoding_train_img = {}\n",
    "for i in train_img:\n",
    "    encoding_train_img[i] = encode_image(i)\n",
    "training_features = encoding_train_img\n",
    "\n",
    "encoding_test_img = {}\n",
    "for i in test_img:\n",
    "    encoding_test_img[i] = encode_image(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_DL = torch.utils.data.DataLoader()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DataLoader 만들기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Captioning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Image_Caption(nn.Module):\n",
    "    def __init__(self, drop_p=0.5, image_dim=256, vocab_size=32000, embedding_dim=4096, hidden_dim=1024):\n",
    "        super(Image_Caption, self).__init__()\n",
    "        self.image_encoder = nn.Sequential(\n",
    "            nn.Dropout(drop_p),\n",
    "            nn.Linear(image_dim, 256),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        self.embedding = embedding_model\n",
    "        self.text_dropout = nn.Dropout(drop_p)\n",
    "        self.linear = nn.Linear(embedding_dim, hidden_dim)\n",
    "        self.lstm = nn.LSTM(hidden_dim, 256, batch_first=True)\n",
    "\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(256, vocab_size),\n",
    "            nn.ReLu(),\n",
    "            nn.Linear(vocab_size, vocab_size),\n",
    "            nn.Softmax(dim=1)\n",
    "        )\n",
    "    def forward(self, image_features, text):\n",
    "        image_features = self.image_encoder(image_features)\n",
    "        with torch.no_grad():\n",
    "            text_features = self.embedding.encode(add_eos(text), batch_size=1, prompt=False, normalize_embeddings=True)\n",
    "        text_features = self.text_dropout(text_features)\n",
    "        text_features = self.linear(text_features)\n",
    "        lstm_out, _ = self.lstm(text_features)\n",
    "        text_features = lstm_out[:, -1, :]\n",
    "        fused_features = image_features + text_features\n",
    "\n",
    "        output = self.decoder(fused_features)\n",
    "        return output\n",
    "    \n",
    "model = Image_Caption()\n",
    "\n",
    "vocab_size = len(ixtoword) + 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_descriptions_100 = {}\n",
    "import os\n",
    "\n",
    "for key, item in train_features.items():\n",
    "  key = os.path.basename(key)\n",
    "  key = key.split(\".\")\n",
    "  train_descriptions_100[key[0]] = train_descriptions[key[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 15\n",
    "batch_size = 3\n",
    "steps_per_epoch = 100\n",
    "generator = data_generator(train_descriptions, train_img, wordtoix, max_length, batch_size)\n",
    "model.fit(generator, epochs=epochs, steps_per_epoch=steps_per_epoch, verbose=1)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
