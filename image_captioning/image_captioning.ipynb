{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install kaggle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instructions for MacOS/Linux Machine\n",
    "!mkdir ~/.kaggle\n",
    "!copy kaggle.json ~/.kaggle/\n",
    "!chmod 600 ~/.kaggle/kaggle.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!kaggle datasets download sakshighadigaonkar/flickr-8k\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!unzip flickr-8k.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer, AutoTokenizer\n",
    "\n",
    "model = SentenceTransformer(\"Alibaba-NLP/gte-Qwen2-7B-instruct\", trust_remote_code=True)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"sentence-transformers/all-MiniLM-L6-v2\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# libraries\n",
    "import numpy as np\n",
    "from numpy import array\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import string\n",
    "import os\n",
    "from PIL import Image\n",
    "from time import time\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_path = \"./flickr-8k/Flickr8k_text/Flickr8k.token.txt\"\n",
    "train_images_path = './flickr-8k/Flickr8k_text/Flickr_8k.trainImages.txt'\n",
    "test_images_path = './flickr-8k/Flickr8k_text/Flickr_8k.testImages.txt'\n",
    "images_path = './flickr-8k/Flickr8k_Dataset/Flicker8k_Dataset'\n",
    "\n",
    "doc = open(token_path,'r').read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "descriptions = dict()\n",
    "for line in doc.split('\\n'):\n",
    "        tokens = line.split()\n",
    "        if len(line) > 2:\n",
    "          image_id = tokens[0].split('.')[0]\n",
    "          image_desc = ' '.join(tokens[1:])\n",
    "          if image_id not in descriptions:\n",
    "              descriptions[image_id] = list()\n",
    "          descriptions[image_id].append(image_desc)\n",
    "vocabulary = set()\n",
    "for key in descriptions.keys():\n",
    "        [vocabulary.update(d.split()) for d in descriptions[key]]\n",
    "print('Original Vocabulary Size: %d words' % len(vocabulary))\n",
    "\n",
    "## Manipulation of the descriptions to prepare them to join them with their correspondent images\n",
    "lines = list()\n",
    "for key, desc_list in descriptions.items():\n",
    "    for desc in desc_list:\n",
    "        lines.append(key + ' ' + desc)\n",
    "new_descriptions = '\\n'.join(lines)\n",
    "\n",
    "## Collects in train the images we are going to use for training purposes.\n",
    "doc = open(train_images_path,'r').read()\n",
    "dataset = list()\n",
    "for line in doc.split('\\n'):\n",
    "    if len(line) > 1:\n",
    "      identifier = line.split('.')[0]\n",
    "      dataset.append(identifier)\n",
    "\n",
    "train = set(dataset)\n",
    "\n",
    "#In this case we are going to actually get the images and stored them in train_img list\n",
    "img = glob.glob(images_path + '/*.jpg')\n",
    "train_images = set(open(train_images_path, 'r').read().strip().split('\\n'))\n",
    "train_img = []\n",
    "for i in img:\n",
    "    if i.split('/')[-1].split('\\\\')[-1] in train_images: ## .split('\\\\')[-1] this is added if you run this in a windows machine. Remove it if you are running in MacOS/Linux machine\n",
    "        train_img.append(i)\n",
    "\n",
    "test_images = set(open(test_images_path, 'r').read().strip().split('\\n'))\n",
    "test_img = []\n",
    "for i in img: \n",
    "    if i.split('/')[-1].split('\\\\')[-1] in test_images:  ## .split('\\\\')[-1] this is added if you run this in a windows machine. Remove it if you are running in MacOS/Linux machine\n",
    "        test_img.append(i)\n",
    "\n",
    "train_descriptions = dict()\n",
    "for line in descriptions.split('\\n'):\n",
    "    tokens = line.split()\n",
    "    image_id, image_desc = tokens[0], tokens[1:]\n",
    "    if image_id in train:\n",
    "        if image_id not in train_descriptions:\n",
    "            train_descriptions[image_id] = list()\n",
    "        desc = '<sos> ' + ' '.join(image_desc) + ' <eos>' # adding the start and end sequence tokens for latter transformation\n",
    "        train_descriptions[image_id].append(desc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Encoding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vit_model = models.vit_b_32(pretrained=True)\n",
    "vit = nn.Sequential(*list(vit_model.children())[:-1]) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_DL = torch.utils.data.DataLoader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "embedding_model = SentenceTransformer('nvidia/NV-Embed-v2', trust_remote_code=True)\n",
    "def add_eos(input_examples):\n",
    "  input_examples = [input_example + embedding_model.tokenizer.eos_token for input_example in input_examples]\n",
    "  return input_examples\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Captioning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Image_Caption(nn.Module):\n",
    "    def __init__(self, drop_p=0.5, image_dim=256, vocab_size=32000, embedding_dim=4096, hidden_dim=1024):\n",
    "        super(Image_Caption, self).__init__()\n",
    "        self.image_encoder = nn.Sequential(\n",
    "            nn.Dropout(drop_p),\n",
    "            nn.Linear(image_dim, 256),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        self.embedding = embedding_model\n",
    "        self.text_dropout = nn.Dropout(drop_p)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)\n",
    "\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, vocab_size),\n",
    "            nn.ReLu(),\n",
    "            nn.Linear(vocab_size, vocab_size),\n",
    "            nn.Softmax(dim=1)\n",
    "        )\n",
    "    def forward(self, image_features, text):\n",
    "        image_features = self.image_encoder(image_features)\n",
    "        with torch.no_grad():\n",
    "            text_features = self.embedding.encode(add_eos(text), batch_size=1, prompt=False, normalize_embeddings=True)\n",
    "        text_features = self.text_dropout(text_features)\n",
    "        lstm_out, _ = self.lstm(text_features)\n",
    "        text_features = lstm_out[:, -1, :]\n",
    "        fused_features = image_features + text_features\n",
    "\n",
    "        output = self.decoder(fused_features)\n",
    "        return output\n",
    "    \n",
    "model = Image_Caption()\n",
    "\n",
    "vacab_size = embedding_model\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
